{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a94fe3",
   "metadata": {},
   "source": [
    "### Submitted by Ashvini Alashetty"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e85f1852",
   "metadata": {},
   "source": [
    "Q1. What are Sequence-to-sequence models?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc.\n",
    "\n",
    "\n",
    "\n",
    "Use Cases of Sequence to sequence models: Sequence to sequence models lies behind numerous systems that you face on a daily basis. For instance, seq2seq model powers applications like Google Translate, voice-enabled devices, and online chatbots. The following are some of the applications:\n",
    "\n",
    "\n",
    "These are only some applications where seq2seq is seen as the best solution. This model can be used as a solution to any sequence-based problem, especially ones where the inputs and outputs have different sizes and categories.\n",
    "\n",
    "Q2. What are the Problem with Vanilla RNNs?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Problems with Vanilla RNNs: RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done.\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is Gradient clipping?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output.\n",
    "\n",
    "Gradient clipping is a technique that tackles exploding gradients. The idea of gradient clipping is very simple: If the gradient gets too large, we rescale it to keep it small. More precisely, if ‖g‖ ≥ c, then\n",
    "\n",
    "g ↤ c · g/‖g‖ where c is a hyperparameter, g is the gradient, and ‖g‖ is the norm of g. Since g/‖g‖ is a unit vector, after rescaling the new g will have norm c. Note that if ‖g‖ < c, then we don’t need to do anything.\n",
    "\n",
    "Gradient clipping ensures the gradient vector g has norm at most c. This helps gradient descent to have a reasonable behaviour even if the loss landscape of the model is irregular. The following figure shows an example with an extremely steep cliff in the loss landscape. Without clipping, the parameters take a huge descent step and leave the “good” region. With clipping, the descent step size is restricted and the parameters stay in the “good” region.\n",
    "\n",
    "This type of learning algorithm is designed based on the way neurons function in the human brain. There are many ways to compute gradient clipping, but a common one is to rescale gradients so that their norm is at most a particular value. With gradient clipping, pre-determined gradient threshold be introduced, and then gradients norms that exceed this threshold are scaled down to match the norm. This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped. There is an introduced bias in the resulting values from the gradient, but gradient clipping can keep things stable.\n",
    "\n",
    "\n",
    "\n",
    "Q4. Explain Attention mechanism\n",
    "\n",
    "Ans:\n",
    "\n",
    "The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation. The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all of the encoded input vectors, with the most relevant vectors being attributed the highest weights.\n",
    "\n",
    "The attention mechanism was introduced by Bahdanau et al. (2014), to address the bottleneck problem that arises with the use of a fixed-length encoding vector, where the decoder would have limited access to the information provided by the input. This is thought to become especially problematic for long and/or complex sequences, where the dimensionality of their representation would be forced to be the same as for shorter or simpler sequences.\n",
    "\n",
    "\n",
    "\n",
    "Similar to the basic encoder-decoder architecture, this fancy mechanism plug a context vector into the gap between encoder and decoder. According to the schematic above, blue represents encoder and red represents decoder; and we could see that context vector takes all cells’ outputs as input to compute the probability distribution of source language words for each single word decoder wants to generate. By utilizing this mechanism, it is possible for decoder to capture somewhat global information rather than solely to infer based on one hidden state.\n",
    "\n",
    "And to build context vector is fairly simple. For a fixed target word, first, we loop over all encoders’ states to compare target and source states to generate scores for each state in encoders. Then we could use softmax to normalize all scores, which generates the probability distribution conditioned on target states. At last, the weights are introduced to make context vector easy to train.\n",
    "\n",
    "Q5. Explain Conditional random fields (CRFs)\n",
    "\n",
    "Ans:\n",
    "\n",
    "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account.\n",
    "\n",
    "Broadly speaking, there are 2 components to the CRF formula:\n",
    "\n",
    "Normalization: You may have observed that there are no probabilities on the right side of the equation where we have the weights and features. However, the output is expected to be a probability and hence there is a need for normalization. The normalization constant Z(x) is a sum of all possible state sequences such that the total becomes 1. You can find more details in the reference section of this article to understand how we arrived at this value.\n",
    "Weights and Features: This component can be thought of as the logistic regression formula with weights and the corresponding features. The weight estimation is performed by maximum likelihood estimation and the features are defined by us.\n",
    "Q6. Explain self-attention Ans:\n",
    "\n",
    "Self Attention, also called intra Attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.\n",
    "\n",
    "We know that word embeddings are vectors that represent the semantic meaning of a word. Words with similar meanings may have similar embeddings. However, in a sentence, the individual meanings of the words do not represent their meanings in the sentence. For example, if I had the phrase, Bank of a river, the embeddings of Bank and river individually mean completely different things, but they have a strong correlation in the sentence. Word embeddings without self-attention do not possess this sense of contextual information, so given the phrase above, a language model would have a low chance of predicting river. In order to address this problem, the self-attention block was proposed in the paper Attention is all you need as part of the original transformer architecture.\n",
    "\n",
    "A self-attention module works by comparing every word in the sentence to every other word in the sentence, including itself, and reweighing the word embeddings of each word to include contextual relevance. It takes in n word embeddings without context and returns n word embeddings with contextual information. For example, in the phrase, Bank of the river, Bank would be compared with Bank, of, the, and river, and as Bank is compared with those four words, its word embedding would be reweighted to include the relevance of the words to its own meaning in the sentence accordingly.\n",
    "\n",
    "Q7. What is Bahdanau Attention?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, irrespective of its length, from which the decoder would then generate a translation. This made it difficult for the neural network to cope with long sentences, essentially resulting in a performance bottleneck.\n",
    "\n",
    "The Bahdanau attention was proposed to address the performance bottleneck of conventional encoder-decoder architectures, achieving significant improvements over the conventional approach.\n",
    "\n",
    "The Bahdanau attention mechanism has inherited its name from the first author of the paper in which it was published.\n",
    "\n",
    "It follows the work of Cho et al. (2014) and Sutskever et al. (2014), who had also employed an RNN encoder-decoder framework for neural machine translation, specifically by encoding a variable-length source sentence into a fixed-length vector. The latter would then be decoded into a variable-length target sentence.\n",
    "\n",
    "Bahdanau et al. (2014) argue that this encoding of a variable-length input into a fixed-length vector squashes the information of the source sentence, irrespective of its length, causing the performance of a basic encoder-decoder model to deteriorate rapidly with an increasing length of the input sentence. The approach they propose, on the other hand, replaces the fixed-length vector with a variable-length one, to improve the translation performance of the basic encoder-decoder model.\n",
    "\n",
    "Q8. What is a Language Model?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions. They are used in natural language processing (NLP) applications, particularly ones that generate text as an output. Some of these applications include , machine translation and question answering.\n",
    "\n",
    "Q9. What is Multi-Head Attention?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies).\n",
    "\n",
    "\n",
    "\n",
    "Q10. What is Bilingual Evaluation Understudy (BLEU)?\n",
    "\n",
    "Ans:\n",
    "\n",
    "BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations.\n",
    "\n",
    "Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.\n",
    "\n",
    "The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence.\n",
    "\n",
    "A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n",
    "\n",
    "The score was developed for evaluating the predictions made by automatic machine translation systems. It is not perfect, but does offer 5 compelling benefits:\n",
    "\n",
    "It is quick and inexpensive to calculate.\n",
    "It is easy to understand.\n",
    "It is language independent.\n",
    "It correlates highly with human evaluation.\n",
    "It has been widely adopted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae4ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
